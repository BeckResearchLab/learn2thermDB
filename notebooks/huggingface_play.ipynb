{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Huggingface\n",
    "The intention of this notebook is only to familiarize with huggigface api and the process of using it, before writing any package or script code using it.\n",
    "\n",
    "## Tests\n",
    "\n",
    "1. Loading a pretrained protein model and making predictions\n",
    "2. Loading a pretrained protein model repurposing the head for sequence generation\n",
    "3. Finetuning a sequence generation model on pairs of sequences\n",
    "4. Pretraining a brand new tokenizer and model on unlabled list of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a pretrained model and making predictions\n",
    "Rostlab pretrained a T5 model on Uniref. for context prediction. It should hypothetically also be used for sequence generation.\n",
    "\n",
    "https://huggingface.co/Rostlab/prot_t5_xl_uniref50\n",
    "\n",
    "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v2\n",
    "\n",
    "The below implementation is exactly the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3fccd3b3f34a3fbe74de9194482bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/238k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795ba7c4f6ef4e398a6f8efb8c658329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90607821acb4a789f4599043057c8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eddf08aa0e4a34852d4f00497d5eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6021bc999e24b4eb3f48c741012296c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/11.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_t5_xl_uniref50 were not used when initializing T5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing T5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', do_lower_case=False)\n",
    "model = T5Model.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_Example = [\"A E T C Z A O\",\"S K T Z P\"]\n",
    "sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.batch_encode_plus(sequences_Example, add_special_tokens=True, padding=True)\n",
    "\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "attention_mask = torch.tensor(ids['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=input_ids,attention_mask=attention_mask,decoder_input_ids=input_ids)\n",
    "\n",
    "# For feature extraction we recommend to use the encoder embedding\n",
    "encoder_embedding = embedding[2].cpu().numpy()\n",
    "decoder_embedding = embedding[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading a pretrained protein model repurposing the head for sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', do_lower_case=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = [\"M S K T Z P\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(input_seq, add_special_tokens=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(ids['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ek/miniconda3/envs/learn2therm/lib/python3.10/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M S K T\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recovered most of the sequence - let's try a more fun one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_seq = [\"AQVINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR\", \"RPRTAFSSEQLARLKREFNENRYLTERRRQQLSSELGLNEAQIKIWFQNKRAKI\"]\n",
    "input_seq = []\n",
    "for s in raw_seq:\n",
    "    new = \"\"\n",
    "    for c in s:\n",
    "        new += f'{c} '\n",
    "    input_seq.append(re.sub(r\"[UZOB]\", \"X\", new))\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', do_lower_case=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "ids = tokenizer(input_seq, add_special_tokens=True, padding=True)\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "output = model.generate(input_ids, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Q V I N T F D G V A D Y L Q T Y H K L P D N Y I T K S E A Q A L G W V A S K G N L A D V A P G K S I G G D I F S N R E G K L P G K S G R T W R E A D I N Y T S G F R N S D R I L Y S S D W L I Y K T', 'R P R T A F S S E Q L A R L K R E F N E N R Y L T E R R R Q Q L S S E L G L N E A Q I K I W F Q N K R A K I']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(o, skip_special_tokens=True) for o in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b9da095881b704967dc48778f25785864cf1780c4a84ddad275ec66ae3d1384"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('learn2therm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
