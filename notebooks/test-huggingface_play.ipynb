{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Huggingface\n",
    "The intention of this notebook is only to familiarize with huggigface api and the process of using it, before writing any package or script code using it.\n",
    "\n",
    "## Tests\n",
    "\n",
    "1. Loading a pretrained protein model and making predictions\n",
    "2. Loading a pretrained protein model repurposing the head for sequence generation\n",
    "3. Finetuning a sequence generation model on pairs of sequences\n",
    "4. Pretraining a brand new tokenizer and model on unlabled list of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. What us the vocabulary of the pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"AQVINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\" \".join(\"AQVINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR\"))['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 111\n"
     ]
    }
   ],
   "source": [
    "print(len(\"AQVINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR\"), len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '</s>': 1,\n",
       " '<unk>': 2,\n",
       " '▁A': 3,\n",
       " '▁L': 4,\n",
       " '▁G': 5,\n",
       " '▁V': 6,\n",
       " '▁S': 7,\n",
       " '▁R': 8,\n",
       " '▁E': 9,\n",
       " '▁D': 10,\n",
       " '▁T': 11,\n",
       " '▁I': 12,\n",
       " '▁P': 13,\n",
       " '▁K': 14,\n",
       " '▁F': 15,\n",
       " '▁Q': 16,\n",
       " '▁N': 17,\n",
       " '▁Y': 18,\n",
       " '▁M': 19,\n",
       " '▁H': 20,\n",
       " '▁W': 21,\n",
       " '▁C': 22,\n",
       " '▁X': 23,\n",
       " '▁B': 24,\n",
       " '▁O': 25,\n",
       " '▁U': 26,\n",
       " '▁Z': 27,\n",
       " '<extra_id_99>': 28,\n",
       " '<extra_id_98>': 29,\n",
       " '<extra_id_97>': 30,\n",
       " '<extra_id_96>': 31,\n",
       " '<extra_id_95>': 32,\n",
       " '<extra_id_94>': 33,\n",
       " '<extra_id_93>': 34,\n",
       " '<extra_id_92>': 35,\n",
       " '<extra_id_91>': 36,\n",
       " '<extra_id_90>': 37,\n",
       " '<extra_id_89>': 38,\n",
       " '<extra_id_88>': 39,\n",
       " '<extra_id_87>': 40,\n",
       " '<extra_id_86>': 41,\n",
       " '<extra_id_85>': 42,\n",
       " '<extra_id_84>': 43,\n",
       " '<extra_id_83>': 44,\n",
       " '<extra_id_82>': 45,\n",
       " '<extra_id_81>': 46,\n",
       " '<extra_id_80>': 47,\n",
       " '<extra_id_79>': 48,\n",
       " '<extra_id_78>': 49,\n",
       " '<extra_id_77>': 50,\n",
       " '<extra_id_76>': 51,\n",
       " '<extra_id_75>': 52,\n",
       " '<extra_id_74>': 53,\n",
       " '<extra_id_73>': 54,\n",
       " '<extra_id_72>': 55,\n",
       " '<extra_id_71>': 56,\n",
       " '<extra_id_70>': 57,\n",
       " '<extra_id_69>': 58,\n",
       " '<extra_id_68>': 59,\n",
       " '<extra_id_67>': 60,\n",
       " '<extra_id_66>': 61,\n",
       " '<extra_id_65>': 62,\n",
       " '<extra_id_64>': 63,\n",
       " '<extra_id_63>': 64,\n",
       " '<extra_id_62>': 65,\n",
       " '<extra_id_61>': 66,\n",
       " '<extra_id_60>': 67,\n",
       " '<extra_id_59>': 68,\n",
       " '<extra_id_58>': 69,\n",
       " '<extra_id_57>': 70,\n",
       " '<extra_id_56>': 71,\n",
       " '<extra_id_55>': 72,\n",
       " '<extra_id_54>': 73,\n",
       " '<extra_id_53>': 74,\n",
       " '<extra_id_52>': 75,\n",
       " '<extra_id_51>': 76,\n",
       " '<extra_id_50>': 77,\n",
       " '<extra_id_49>': 78,\n",
       " '<extra_id_48>': 79,\n",
       " '<extra_id_47>': 80,\n",
       " '<extra_id_46>': 81,\n",
       " '<extra_id_45>': 82,\n",
       " '<extra_id_44>': 83,\n",
       " '<extra_id_43>': 84,\n",
       " '<extra_id_42>': 85,\n",
       " '<extra_id_41>': 86,\n",
       " '<extra_id_40>': 87,\n",
       " '<extra_id_39>': 88,\n",
       " '<extra_id_38>': 89,\n",
       " '<extra_id_37>': 90,\n",
       " '<extra_id_36>': 91,\n",
       " '<extra_id_35>': 92,\n",
       " '<extra_id_34>': 93,\n",
       " '<extra_id_33>': 94,\n",
       " '<extra_id_32>': 95,\n",
       " '<extra_id_31>': 96,\n",
       " '<extra_id_30>': 97,\n",
       " '<extra_id_29>': 98,\n",
       " '<extra_id_28>': 99,\n",
       " '<extra_id_27>': 100,\n",
       " '<extra_id_26>': 101,\n",
       " '<extra_id_25>': 102,\n",
       " '<extra_id_24>': 103,\n",
       " '<extra_id_23>': 104,\n",
       " '<extra_id_22>': 105,\n",
       " '<extra_id_21>': 106,\n",
       " '<extra_id_20>': 107,\n",
       " '<extra_id_19>': 108,\n",
       " '<extra_id_18>': 109,\n",
       " '<extra_id_17>': 110,\n",
       " '<extra_id_16>': 111,\n",
       " '<extra_id_15>': 112,\n",
       " '<extra_id_14>': 113,\n",
       " '<extra_id_13>': 114,\n",
       " '<extra_id_12>': 115,\n",
       " '<extra_id_11>': 116,\n",
       " '<extra_id_10>': 117,\n",
       " '<extra_id_9>': 118,\n",
       " '<extra_id_8>': 119,\n",
       " '<extra_id_7>': 120,\n",
       " '<extra_id_6>': 121,\n",
       " '<extra_id_5>': 122,\n",
       " '<extra_id_4>': 123,\n",
       " '<extra_id_3>': 124,\n",
       " '<extra_id_2>': 125,\n",
       " '<extra_id_1>': 126,\n",
       " '<extra_id_0>': 127}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a pretrained model and making predictions\n",
    "Rostlab pretrained a T5 model on Uniref. for context prediction. It should hypothetically also be used for sequence generation.\n",
    "\n",
    "https://huggingface.co/Rostlab/prot_t5_xl_uniref50\n",
    "\n",
    "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v2\n",
    "\n",
    "The below implementation is exactly the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_t5_xl_uniref50 were not used when initializing T5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing T5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', do_lower_case=False)\n",
    "model = T5Model.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_Example = [\"A E T C Z A O\",\"S K T Z P\"]\n",
    "sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.batch_encode_plus(sequences_Example, add_special_tokens=True, padding=True)\n",
    "\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "attention_mask = torch.tensor(ids['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=input_ids,attention_mask=attention_mask,decoder_input_ids=input_ids)\n",
    "\n",
    "# For feature extraction we recommend to use the encoder embedding\n",
    "encoder_embedding = embedding[2].cpu().numpy()\n",
    "decoder_embedding = embedding[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer from the Rostlab model does not token aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading a pretrained protein model repurposing the head for sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', do_lower_case=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = [\"M S K T Z P\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(input_seq, add_special_tokens=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(ids['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ek/miniconda3/envs/learn2therm/lib/python3.10/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M S K T\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recovered most of the sequence - let's try a more fun one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_seq = [\"AQVINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR\", \"RPRTAFSSEQLARLKREFNENRYLTERRRQQLSSELGLNEAQIKIWFQNKRAKI\"]\n",
    "input_seq = []\n",
    "for s in raw_seq:\n",
    "    new = \"\"\n",
    "    for c in s:\n",
    "        new += f'{c} '\n",
    "    input_seq.append(re.sub(r\"[UZOB]\", \"X\", new))\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', do_lower_case=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "ids = tokenizer(input_seq, add_special_tokens=True, padding=True)\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "output = model.generate(input_ids, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Q V I N T F D G V A D Y L Q T Y H K L P D N Y I T K S E A Q A L G W V A S K G N L A D V A P G K S I G G D I F S N R E G K L P G K S G R T W R E A D I N Y T S G F R N S D R I L Y S S D W L I Y K T', 'R P R T A F S S E Q L A R L K R E F N E N R Y L T E R R R Q Q L S S E L G L N E A Q I K I W F Q N K R A K I']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(o, skip_special_tokens=True) for o in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b9da095881b704967dc48778f25785864cf1780c4a84ddad275ec66ae3d1384"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('learn2therm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
